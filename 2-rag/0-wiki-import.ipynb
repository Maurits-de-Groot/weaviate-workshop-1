{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import weaviate, os\n",
    "\n",
    "# # Connect to the local instance deployed with Docker Compose\n",
    "# client = weaviate.connect_to_local(\n",
    "#     headers={\n",
    "#         # \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\"), # Replace with your inference API key\n",
    "#         # \"X-Cohere-Api-Key\": os.getenv(\"COHERE_API_KEY\"), # Replace with your inference API key\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# client.is_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate, os\n",
    "\n",
    "# Connect to a cloud instance of Weaviate (with WCS)\n",
    "client = weaviate.connect_to_wcs(\n",
    "    cluster_url=os.getenv(\"WORKSHOP_DEMO_URL\"),\n",
    "    auth_credentials=weaviate.auth.AuthApiKey(os.getenv(\"WORKSHOP_DEMO_KEY_ADMIN\")),\n",
    ")\n",
    "\n",
    "client.is_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.config import Configure, Property, DataType\n",
    "\n",
    "if client.collections.exists(\"Wikipedia\"):\n",
    "    print(\"Collection already exists\")\n",
    "    inp = input('Do you want to recreate \"Wikipedia\" collection? Say Y to confirm.')\n",
    "    if (inp == \"Y\"):\n",
    "        client.collections.delete(\"Wikipedia\")\n",
    "    else:\n",
    "        exit\n",
    "\n",
    "# Create a collection here - with Cohere as a vectorizer\n",
    "client.collections.create(\n",
    "    name=\"Wikipedia\",\n",
    "    # Swap this\n",
    "    # vectorizer_config=[\n",
    "    #     Configure.NamedVectors.text2vec_cohere(\n",
    "    #         name=\"text_vector\",\n",
    "    #         model: \"embed-multilingual-v2.0\",\n",
    "    #         source_properties=[\"text\"]\n",
    "    # )],\n",
    "    vectorizer_config=Configure.Vectorizer.text2vec_cohere(\n",
    "        model=\"embed-multilingual-v2.0\"\n",
    "    ),\n",
    "\n",
    "    generative_config=Configure.Generative.openai(\"gpt-4\"),\n",
    "\n",
    "    properties=[\n",
    "        Property(name=\"text\", data_type=DataType.TEXT),\n",
    "        Property(name=\"title\", data_type=DataType.TEXT, skip_vectorization=True),\n",
    "        Property(name=\"wiki_id\", data_type=DataType.INT, skip_vectorization=True),\n",
    "        Property(name=\"url\", data_type=DataType.TEXT, skip_vectorization=True),\n",
    "        Property(name=\"lang\", data_type=DataType.TEXT, skip_vectorization=True),\n",
    "        Property(name=\"lang_id\", data_type=DataType.INT, skip_vectorization=True),\n",
    "        Property(name=\"views\", data_type=DataType.NUMBER, skip_vectorization=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebawita/github/weaviate-workshop/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def import_wiki_data(lang, lang_id, max_rows, skip_rows=0):\n",
    "    print(f'Importing {max_rows} data items for {lang}')\n",
    "\n",
    "    dataset = load_dataset(f'Cohere/wikipedia-22-12-{lang}-embeddings', split='train', streaming=True)\n",
    "    dataset = dataset.skip(skip_rows)\n",
    "\n",
    "    # counter = 0\n",
    "    counter = skip_rows\n",
    "\n",
    "    wikipedia = client.collections.get(\"Wikipedia\")\n",
    "\n",
    "    with wikipedia.batch.fixed_size(batch_size=1000, concurrent_requests=4) as batch:\n",
    "        for item in tqdm(dataset, initial=skip_rows, total=max_rows):\n",
    "            vector = item['emb']\n",
    "            data_to_insert = {   \n",
    "                # '_id': doc_id,\n",
    "                'text': item['text'],\n",
    "                'wiki_id': item['wiki_id'],\n",
    "                'title': item['title'],\n",
    "                'url': item['url'],\n",
    "                'views': item['views'],\n",
    "                'lang': lang,\n",
    "                'lang_id': lang_id,\n",
    "            }\n",
    "\n",
    "            batch.add_object(\n",
    "                properties=data_to_insert,\n",
    "                # Swap this\n",
    "                # vector={\n",
    "                #     \"text_vector\": vector   \n",
    "                # }\n",
    "                vector=vector\n",
    "            )\n",
    "            \n",
    "            counter += 1\n",
    "            if counter >= max_rows:\n",
    "                break\n",
    "\n",
    "            if (counter % 200 == 0):\n",
    "                # this doesn't seem to see the errors, that are later visible in the final check\n",
    "                if (len(wikipedia.batch.failed_objects)>0):\n",
    "                    print(f\"Some errors {len(wikipedia.batch.failed_objects)}\")\n",
    "                    print(wikipedia.batch.failed_objects[-1])\n",
    "    \n",
    "    # check for errors at the end\n",
    "    if (len(wikipedia.batch.failed_objects)>0):\n",
    "        print(\"Final error check\")\n",
    "        print(f\"Some errors {len(wikipedia.batch.failed_objects)}\")\n",
    "        print(wikipedia.batch.failed_objects[-1])\n",
    "    \n",
    "    print(f'Imported {counter} items for {lang}')\n",
    "    print( '-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing 5000000 data items for en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 1737979/5000000 [3:42:26<1:56:22, 467.18it/s]  '(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: c90520bd-1690-4fbb-8ff4-b1f0b7243aec)')' thrown while requesting GET https://huggingface.co/datasets/Cohere/wikipedia-22-12-en-embeddings/resolve/85c2eca83d4b9dcecc043c23748cb8c1047f683f/data/train-00012-of-00253-4917d4a02ad59415.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: e956d06c-e974-4438-9937-043c631d6c22)')' thrown while requesting GET https://huggingface.co/datasets/Cohere/wikipedia-22-12-en-embeddings/resolve/85c2eca83d4b9dcecc043c23748cb8c1047f683f/data/train-00012-of-00253-4917d4a02ad59415.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      " 35%|███▌      | 1768009/5000000 [5:50:47<2:02:39, 439.15it/s]  "
     ]
    }
   ],
   "source": [
    "# import_per_country = 250\n",
    "# import_per_country = 500_000\n",
    "# import_per_country = 1_000_000\n",
    "import_per_country = 5_000_000\n",
    "\n",
    "import_wiki_data('en', 0, import_per_country, 0)\n",
    "# import_wiki_data('de', 1, import_per_country, 0)\n",
    "# import_wiki_data('fr', 2, import_per_country, 0)\n",
    "# import_wiki_data('es', 3, import_per_country, 0)\n",
    "# import_wiki_data('it', 4, import_per_country, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
